"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.ComprehendFilterAgent = void 0;
const agent_1 = require("./agent");
const types_1 = require("../types");
const logger_1 = require("../utils/logger");
const client_comprehend_1 = require("@aws-sdk/client-comprehend");
/**
 * ComprehendContentFilterAgent class
 *
 * This agent uses Amazon Comprehend to analyze and filter content based on
 * sentiment, PII, and toxicity. It can be configured to enable/disable specific
 * checks and allows for the addition of custom checks.
 */
class ComprehendFilterAgent extends agent_1.Agent {
    /**
     * Constructor for ComprehendContentFilterAgent
     * @param options - Configuration options for the agent
     */
    constructor(options) {
        var _a, _b, _c, _d, _e, _f, _g;
        super(options);
        this.customChecks = [];
        this.comprehendClient = options.region
            ? new client_comprehend_1.ComprehendClient({ region: options.region })
            : new client_comprehend_1.ComprehendClient();
        // Set default configuration using fields from options
        this.enableSentimentCheck = (_a = options.enableSentimentCheck) !== null && _a !== void 0 ? _a : true;
        this.enablePiiCheck = (_b = options.enablePiiCheck) !== null && _b !== void 0 ? _b : true;
        this.enableToxicityCheck = (_c = options.enableToxicityCheck) !== null && _c !== void 0 ? _c : true;
        this.sentimentThreshold = (_d = options.sentimentThreshold) !== null && _d !== void 0 ? _d : 0.7;
        this.toxicityThreshold = (_e = options.toxicityThreshold) !== null && _e !== void 0 ? _e : 0.7;
        this.allowPii = (_f = options.allowPii) !== null && _f !== void 0 ? _f : false;
        this.languageCode = (_g = this.validateLanguageCode(options.languageCode)) !== null && _g !== void 0 ? _g : 'en';
        // Ensure at least one check is enabled
        if (!this.enableSentimentCheck &&
            !this.enablePiiCheck &&
            !this.enableToxicityCheck) {
            this.enableToxicityCheck = true;
        }
    }
    /**
     * Processes a user request by sending it to the Amazon Bedrock agent for processing.
     * @param inputText - The user input as a string.
     * @param userId - The ID of the user sending the request.
     * @param sessionId - The ID of the session associated with the conversation.
     * @param chatHistory - An array of Message objects representing the conversation history.
     * @param additionalParams - Optional additional parameters as key-value pairs.
     * @returns A Promise that resolves to a Message object containing the agent's response.
     */
    /* eslint-disable @typescript-eslint/no-unused-vars */
    async processRequest(inputText, userId, sessionId, chatHistory, additionalParams) {
        try {
            const issues = [];
            // Run all checks in parallel
            const [sentimentResult, piiResult, toxicityResult] = await Promise.all([
                this.enableSentimentCheck ? this.detectSentiment(inputText) : null,
                this.enablePiiCheck ? this.detectPiiEntities(inputText) : null,
                this.enableToxicityCheck ? this.detectToxicContent(inputText) : null
            ]);
            // Process results
            if (this.enableSentimentCheck && sentimentResult) {
                const sentimentIssue = this.checkSentiment(sentimentResult);
                if (sentimentIssue)
                    issues.push(sentimentIssue);
            }
            if (this.enablePiiCheck && piiResult) {
                const piiIssue = this.checkPii(piiResult);
                if (piiIssue)
                    issues.push(piiIssue);
            }
            if (this.enableToxicityCheck && toxicityResult) {
                const toxicityIssue = this.checkToxicity(toxicityResult);
                if (toxicityIssue)
                    issues.push(toxicityIssue);
            }
            // Run custom checks
            for (const check of this.customChecks) {
                const customIssue = await check(inputText);
                if (customIssue)
                    issues.push(customIssue);
            }
            if (issues.length > 0) {
                logger_1.Logger.logger.warn(`Content filter issues detected: ${issues.join('; ')}`);
                return null; // Return null to indicate content should not be processed further
            }
            // If no issues, return the original input as a ConversationMessage
            return {
                role: types_1.ParticipantRole.ASSISTANT,
                content: [{ text: inputText }]
            };
        }
        catch (error) {
            logger_1.Logger.logger.error("Error in ComprehendContentFilterAgent:", error);
            throw error;
        }
    }
    /**
     * Add a custom check function to the agent
     * @param check - A function that takes a string input and returns a Promise<string | null>
     */
    addCustomCheck(check) {
        this.customChecks.push(check);
    }
    /**
     * Check sentiment of the input text
     * @param result - Result from Comprehend's sentiment detection
     * @returns A string describing the issue if sentiment is negative, null otherwise
     */
    checkSentiment(result) {
        var _a;
        if (result.Sentiment === 'NEGATIVE' &&
            ((_a = result.SentimentScore) === null || _a === void 0 ? void 0 : _a.Negative) > this.sentimentThreshold) {
            return `Negative sentiment detected (${result.SentimentScore.Negative.toFixed(2)})`;
        }
        return null;
    }
    /**
     * Check for PII in the input text
     * @param result - Result from Comprehend's PII detection
     * @returns A string describing the issue if PII is detected, null otherwise
     */
    checkPii(result) {
        if (!this.allowPii && result.Entities && result.Entities.length > 0) {
            return `PII detected: ${result.Entities.map(e => e.Type).join(', ')}`;
        }
        return null;
    }
    /**
     * Check for toxic content in the input text
     * @param result - Result from Comprehend's toxic content detection
     * @returns A string describing the issue if toxic content is detected, null otherwise
     */
    checkToxicity(result) {
        const toxicLabels = this.getToxicLabels(result);
        if (toxicLabels.length > 0) {
            return `Toxic content detected: ${toxicLabels.join(', ')}`;
        }
        return null;
    }
    /**
     * Detect sentiment using Amazon Comprehend
     * @param text - Input text to analyze
     */
    async detectSentiment(text) {
        const command = new client_comprehend_1.DetectSentimentCommand({
            Text: text,
            LanguageCode: this.languageCode
        });
        return this.comprehendClient.send(command);
    }
    /**
     * Detect PII entities using Amazon Comprehend
     * @param text - Input text to analyze
     */
    async detectPiiEntities(text) {
        const command = new client_comprehend_1.DetectPiiEntitiesCommand({
            Text: text,
            LanguageCode: this.languageCode
        });
        return this.comprehendClient.send(command);
    }
    /**
     * Detect toxic content using Amazon Comprehend
     * @param text - Input text to analyze
     */
    async detectToxicContent(text) {
        const command = new client_comprehend_1.DetectToxicContentCommand({
            TextSegments: [{ Text: text }],
            LanguageCode: this.languageCode
        });
        return this.comprehendClient.send(command);
    }
    /**
     * Extract toxic labels from the Comprehend response
     * @param toxicityResult - Result from Comprehend's toxic content detection
     * @returns Array of toxic label names that exceed the threshold
     */
    getToxicLabels(toxicityResult) {
        const toxicLabels = [];
        if (toxicityResult.ResultList && Array.isArray(toxicityResult.ResultList)) {
            toxicityResult.ResultList.forEach((result) => {
                if (result.Labels && Array.isArray(result.Labels)) {
                    result.Labels.forEach((label) => {
                        if (label.Score > this.toxicityThreshold) {
                            toxicLabels.push(label.Name);
                        }
                    });
                }
            });
        }
        return toxicLabels;
    }
    /**
     * Set the language code for Comprehend operations
     * @param languageCode - The ISO 639-1 language code
     */
    setLanguageCode(languageCode) {
        const validatedLanguageCode = this.validateLanguageCode(languageCode);
        if (validatedLanguageCode) {
            this.languageCode = validatedLanguageCode;
        }
        else {
            throw new Error(`Invalid language code: ${languageCode}`);
        }
    }
    /**
     * Validate the provided language code
     * @param languageCode - The language code to validate
     * @returns The validated LanguageCode or undefined if invalid
     */
    validateLanguageCode(languageCode) {
        if (!languageCode)
            return undefined;
        const validLanguageCodes = [
            'en', 'es', 'fr', 'de', 'it', 'pt', 'ar', 'hi', 'ja', 'ko', 'zh', 'zh-TW'
        ];
        return validLanguageCodes.includes(languageCode) ? languageCode : undefined;
    }
}
exports.ComprehendFilterAgent = ComprehendFilterAgent;
//# sourceMappingURL=comprehendFilterAgent.js.map