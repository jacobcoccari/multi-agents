"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
exports.BedrockLLMAgent = void 0;
const client_bedrock_runtime_1 = require("@aws-sdk/client-bedrock-runtime");
const agent_1 = require("./agent");
const types_1 = require("../types");
const logger_1 = require("../utils/logger");
/**
 * BedrockAgent class represents an agent that uses Amazon Bedrock for natural language processing.
 * It extends the base Agent class and implements the processRequest method using Bedrock's API.
 */
class BedrockLLMAgent extends agent_1.Agent {
    /**
     * Constructs a new BedrockAgent instance.
     * @param options - Configuration options for the agent, inherited from AgentOptions.
     */
    constructor(options) {
        var _a, _b, _c, _d, _e, _f;
        super(options);
        this.defaultMaxRecursions = 20;
        this.client = options.client ? options.client : options.region
            ? new client_bedrock_runtime_1.BedrockRuntimeClient({ region: options.region })
            : new client_bedrock_runtime_1.BedrockRuntimeClient();
        // Initialize the modelId
        this.modelId = (_a = options.modelId) !== null && _a !== void 0 ? _a : types_1.BEDROCK_MODEL_ID_CLAUDE_3_HAIKU;
        this.streaming = (_b = options.streaming) !== null && _b !== void 0 ? _b : false;
        this.inferenceConfig = (_c = options.inferenceConfig) !== null && _c !== void 0 ? _c : {};
        this.guardrailConfig = (_d = options.guardrailConfig) !== null && _d !== void 0 ? _d : null;
        this.retriever = (_e = options.retriever) !== null && _e !== void 0 ? _e : null;
        this.toolConfig = (_f = options.toolConfig) !== null && _f !== void 0 ? _f : null;
        this.promptTemplate = `You are a ${this.name}. ${this.description} Provide helpful and accurate information based on your expertise.
    You will engage in an open-ended conversation, providing helpful and accurate information based on your expertise.
    The conversation will proceed as follows:
    - The human may ask an initial question or provide a prompt on any topic.
    - You will provide a relevant and informative response.
    - The human may then follow up with additional questions or prompts related to your previous response, allowing for a multi-turn dialogue on that topic.
    - Or, the human may switch to a completely new and unrelated topic at any point.
    - You will seamlessly shift your focus to the new topic, providing thoughtful and coherent responses based on your broad knowledge base.
    Throughout the conversation, you should aim to:
    - Understand the context and intent behind each new question or prompt.
    - Provide substantive and well-reasoned responses that directly address the query.
    - Draw insights and connections from your extensive knowledge when appropriate.
    - Ask for clarification if any part of the question or prompt is ambiguous.
    - Maintain a consistent, respectful, and engaging tone tailored to the human's communication style.
    - Seamlessly transition between topics as the human introduces new subjects.`;
        if (options.customSystemPrompt) {
            this.setSystemPrompt(options.customSystemPrompt.template, options.customSystemPrompt.variables);
        }
    }
    /**
     * Abstract method to process a request.
     * This method must be implemented by all concrete agent classes.
     *
     * @param inputText - The user input as a string.
     * @param chatHistory - An array of Message objects representing the conversation history.
     * @param additionalParams - Optional additional parameters as key-value pairs.
     * @returns A Promise that resolves to a Message object containing the agent's response.
     */
    /* eslint-disable @typescript-eslint/no-unused-vars */
    async processRequest(inputText, userId, sessionId, chatHistory, additionalParams) {
        var _a, _b;
        // Construct the user's message based on the provided inputText
        const userMessage = {
            role: types_1.ParticipantRole.USER,
            content: [{ text: `${inputText}` }],
        };
        // Combine the existing chat history with the user's message
        const conversation = [
            ...chatHistory,
            userMessage,
        ];
        this.updateSystemPrompt();
        let systemPrompt = this.systemPrompt;
        // Update the system prompt with the latest history, agent descriptions, and custom variables
        if (this.retriever) {
            // retrieve from Vector store
            const response = await this.retriever.retrieveAndCombineResults(inputText);
            const contextPrompt = "\nHere is the context to use to answer the user's question:\n" +
                response;
            systemPrompt = systemPrompt + contextPrompt;
        }
        // Prepare the command to converse with the Bedrock API
        const converseCmd = {
            modelId: this.modelId,
            messages: conversation, //Include the updated conversation history
            system: [{ text: systemPrompt }],
            inferenceConfig: {
                maxTokens: this.inferenceConfig.maxTokens,
                temperature: this.inferenceConfig.temperature,
                topP: this.inferenceConfig.topP,
                stopSequences: this.inferenceConfig.stopSequences,
            },
            guardrailConfig: this.guardrailConfig ? this.guardrailConfig : undefined,
            toolConfig: (this.toolConfig ? { tools: this.toolConfig.tool } : undefined)
        };
        if (this.streaming) {
            return this.handleStreamingResponse(converseCmd);
        }
        else {
            let continueWithTools = false;
            let finalMessage = { role: types_1.ParticipantRole.USER, content: [] };
            let maxRecursions = ((_a = this.toolConfig) === null || _a === void 0 ? void 0 : _a.toolMaxRecursions) || this.defaultMaxRecursions;
            do {
                // send the conversation to Amazon Bedrock
                const bedrockResponse = await this.handleSingleResponse(converseCmd);
                // Append the model's response to the ongoing conversation
                conversation.push(bedrockResponse);
                // process model response
                if ((_b = bedrockResponse === null || bedrockResponse === void 0 ? void 0 : bedrockResponse.content) === null || _b === void 0 ? void 0 : _b.some((content) => 'toolUse' in content)) {
                    // forward everything to the tool use handler
                    if (!this.toolConfig) {
                        throw new Error("Tool config is not defined");
                    }
                    const toolResponse = await this.toolConfig.useToolHandler(bedrockResponse, conversation);
                    continueWithTools = true;
                    converseCmd.messages.push(toolResponse);
                }
                else {
                    continueWithTools = false;
                    finalMessage = bedrockResponse;
                }
                maxRecursions--;
                converseCmd.messages = conversation;
            } while (continueWithTools && maxRecursions > 0);
            return finalMessage;
        }
    }
    async handleSingleResponse(input) {
        try {
            const command = new client_bedrock_runtime_1.ConverseCommand(input);
            const response = await this.client.send(command);
            if (!response.output) {
                throw new Error("No output received from Bedrock model");
            }
            return response.output.message;
        }
        catch (error) {
            logger_1.Logger.logger.error("Error invoking Bedrock model:", error);
            throw error;
        }
    }
    async *handleStreamingResponse(input) {
        var _a, _b, _c, _d, _e, _f, _g, _h, _j;
        let toolBlock = { toolUseId: '', input: {}, name: '' };
        let inputString = '';
        let toolUse = false;
        let recursions = ((_a = this.toolConfig) === null || _a === void 0 ? void 0 : _a.toolMaxRecursions) || this.defaultMaxRecursions;
        try {
            do {
                const command = new client_bedrock_runtime_1.ConverseStreamCommand(input);
                const response = await this.client.send(command);
                if (!response.stream) {
                    throw new Error("No stream received from Bedrock model");
                }
                for await (const chunk of response.stream) {
                    if (chunk.contentBlockDelta && chunk.contentBlockDelta.delta && chunk.contentBlockDelta.delta.text) {
                        yield chunk.contentBlockDelta.delta.text;
                    }
                    else if ((_c = (_b = chunk.contentBlockStart) === null || _b === void 0 ? void 0 : _b.start) === null || _c === void 0 ? void 0 : _c.toolUse) {
                        toolBlock = (_e = (_d = chunk.contentBlockStart) === null || _d === void 0 ? void 0 : _d.start) === null || _e === void 0 ? void 0 : _e.toolUse;
                    }
                    else if ((_g = (_f = chunk.contentBlockDelta) === null || _f === void 0 ? void 0 : _f.delta) === null || _g === void 0 ? void 0 : _g.toolUse) {
                        inputString += chunk.contentBlockDelta.delta.toolUse.input;
                    }
                    else if (((_h = chunk.messageStop) === null || _h === void 0 ? void 0 : _h.stopReason) === 'tool_use') {
                        toolBlock.input = JSON.parse(inputString);
                        const message = { role: types_1.ParticipantRole.ASSISTANT, content: [{ 'toolUse': toolBlock }] };
                        input.messages.push(message);
                        const toolResponse = await this.toolConfig.useToolHandler(message, input.messages);
                        input.messages.push(toolResponse);
                        toolUse = true;
                    }
                    else if (((_j = chunk.messageStop) === null || _j === void 0 ? void 0 : _j.stopReason) === 'end_turn') {
                        toolUse = false;
                    }
                }
            } while (toolUse && --recursions > 0);
        }
        catch (error) {
            logger_1.Logger.logger.error("Error getting stream from Bedrock model:", error);
            throw error;
        }
    }
    setSystemPrompt(template, variables) {
        if (template) {
            this.promptTemplate = template;
        }
        if (variables) {
            this.customVariables = variables;
        }
        this.updateSystemPrompt();
    }
    updateSystemPrompt() {
        const allVariables = {
            ...this.customVariables
        };
        this.systemPrompt = this.replaceplaceholders(this.promptTemplate, allVariables);
    }
    replaceplaceholders(template, variables) {
        return template.replace(/{{(\w+)}}/g, (match, key) => {
            if (key in variables) {
                const value = variables[key];
                if (Array.isArray(value)) {
                    return value.join("\n");
                }
                return value;
            }
            return match; // If no replacement found, leave the placeholder as is
        });
    }
}
exports.BedrockLLMAgent = BedrockLLMAgent;
//# sourceMappingURL=bedrockLLMAgent.js.map